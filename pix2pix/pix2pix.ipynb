{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theovincent/coach/blob/master/pix2pix/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wNjDKdQy35h"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRm-USlsHgEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e14af7a0-7886-420c-ab58-005118a411a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2443, done.\u001b[K\n",
            "remote: Total 2443 (delta 0), reused 0 (delta 0), pack-reused 2443\u001b[K\n",
            "Receiving objects: 100% (2443/2443), 8.18 MiB | 21.15 MiB/s, done.\n",
            "Resolving deltas: 100% (1532/1532), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt3igws3eiVp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1EySlOXwwoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d780c836-8ad2-4097-c439-c831ffa485b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.11.1+cu111)\n",
            "Collecting dominate>=2.4.0\n",
            "  Downloading dominate-2.6.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting visdom>=0.1.8.8\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[K     |████████████████████████████████| 676 kB 10.6 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (22.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.15.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 9.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.17.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.1-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 68.6 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.25-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 77.4 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb->-r requirements.txt (line 5)) (1.1.0)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Building wheels for collected packages: visdom, subprocess32, pathtools, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=f0af771fe3e046b9af3cfabf7fcb00774dcb5755fc65ff4d04aa51cac1bd6ae1\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=7c11988f6ca86efc1a8babca7d701be48991819cfb4bd1e6afb46a2c87d1bb6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=e36482991fed2022cae7f239e02e514ffb9e559180bc6346c50da63cecc50ca8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5710 sha256=74c8ad4d9903e786d1017ff8042309b45bc082552a6306afc5bbee3bf19e2cd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built visdom subprocess32 pathtools torchfile\n",
            "Installing collected packages: smmap, jsonpointer, gitdb, yaspin, websocket-client, torchfile, subprocess32, shortuuid, sentry-sdk, pathtools, jsonpatch, GitPython, docker-pycreds, configparser, wandb, visdom, dominate\n",
            "Successfully installed GitPython-3.1.25 configparser-5.2.0 docker-pycreds-0.4.0 dominate-2.6.0 gitdb-4.0.9 jsonpatch-1.32 jsonpointer-2.2 pathtools-0.1.2 sentry-sdk-1.5.1 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 torchfile-0.1.0 visdom-0.1.8.9 wandb-0.12.9 websocket-client-1.2.3 yaspin-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8daqlgVhw29P"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "Download one of the official datasets with:\n",
        "\n",
        "-   `bash ./datasets/download_pix2pix_dataset.sh [cityscapes, night2day, edges2handbags, edges2shoes, facades, maps]`\n",
        "\n",
        "Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('Drive')\n",
        "# Adapt your own path here\n",
        "path_to_folder=\"/content/pytorch-CycleGAN-and-pix2pix/Drive/MyDrive/MVA/ObjectRecognition/LearningToAct/Doom_Health_Supreme_Split\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ElDoXm5ugrT",
        "outputId": "ff5323aa-c6bf-4d44-ab7c-1dcfd1aebd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python ./datasets/combine_A_and_B.py --fold_A $path_to_folder/A --fold_B $path_to_folder/B --fold_AB Doom_Health_Supreme_Split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu30gLczxbiA",
        "outputId": "0b6c9806-ca27-49ba-c53e-cc8dd06ba9ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold_A] =  /content/pytorch-CycleGAN-and-pix2pix/Drive/MyDrive/MVA/ObjectRecognition/LearningToAct/Doom_Health_Supreme_Split/A\n",
            "[fold_B] =  /content/pytorch-CycleGAN-and-pix2pix/Drive/MyDrive/MVA/ObjectRecognition/LearningToAct/Doom_Health_Supreme_Split/B\n",
            "[fold_AB] =  Doom_Heatlh_Supreme_Split\n",
            "[num_imgs] =  1000000\n",
            "[use_AB] =  False\n",
            "[no_multiprocessing] =  False\n",
            "split = train, use 2001/2001 images\n",
            "split = train, number of images = 2001\n",
            "split = test, use 500/500 images\n",
            "split = test, number of images = 500\n",
            "split = val, use 500/500 images\n",
            "split = val, number of images = 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFw1kDQBx3LN"
      },
      "source": [
        "# Training\n",
        "\n",
        "-   `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA`\n",
        "\n",
        "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. Add `--direction BtoA` if you want to train a model to transfrom from class B to A."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ_8BXu3EYtR",
        "outputId": "c34a599a-bb5a-4ba7-bddb-45bd275b91e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: train.py [-h] --dataroot DATAROOT [--name NAME] [--use_wandb]\n",
            "                [--gpu_ids GPU_IDS] [--checkpoints_dir CHECKPOINTS_DIR]\n",
            "                [--model MODEL] [--input_nc INPUT_NC] [--output_nc OUTPUT_NC]\n",
            "                [--ngf NGF] [--ndf NDF] [--netD NETD] [--netG NETG]\n",
            "                [--n_layers_D N_LAYERS_D] [--norm NORM]\n",
            "                [--init_type INIT_TYPE] [--init_gain INIT_GAIN] [--no_dropout]\n",
            "                [--dataset_mode DATASET_MODE] [--direction DIRECTION]\n",
            "                [--serial_batches] [--num_threads NUM_THREADS]\n",
            "                [--batch_size BATCH_SIZE] [--load_size LOAD_SIZE]\n",
            "                [--crop_size CROP_SIZE] [--max_dataset_size MAX_DATASET_SIZE]\n",
            "                [--preprocess PREPROCESS] [--no_flip]\n",
            "                [--display_winsize DISPLAY_WINSIZE] [--epoch EPOCH]\n",
            "                [--load_iter LOAD_ITER] [--verbose] [--suffix SUFFIX]\n",
            "                [--display_freq DISPLAY_FREQ] [--display_ncols DISPLAY_NCOLS]\n",
            "                [--display_id DISPLAY_ID] [--display_server DISPLAY_SERVER]\n",
            "                [--display_env DISPLAY_ENV] [--display_port DISPLAY_PORT]\n",
            "                [--update_html_freq UPDATE_HTML_FREQ]\n",
            "                [--print_freq PRINT_FREQ] [--no_html]\n",
            "                [--save_latest_freq SAVE_LATEST_FREQ]\n",
            "                [--save_epoch_freq SAVE_EPOCH_FREQ] [--save_by_iter]\n",
            "                [--continue_train] [--epoch_count EPOCH_COUNT] [--phase PHASE]\n",
            "                [--n_epochs N_EPOCHS] [--n_epochs_decay N_EPOCHS_DECAY]\n",
            "                [--beta1 BETA1] [--lr LR] [--gan_mode GAN_MODE]\n",
            "                [--pool_size POOL_SIZE] [--lr_policy LR_POLICY]\n",
            "                [--lr_decay_iters LR_DECAY_ITERS]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --dataroot DATAROOT   path to images (should have subfolders trainA, trainB,\n",
            "                        valA, valB, etc) (default: None)\n",
            "  --name NAME           name of the experiment. It decides where to store\n",
            "                        samples and models (default: experiment_name)\n",
            "  --use_wandb           use wandb (default: False)\n",
            "  --gpu_ids GPU_IDS     gpu ids: e.g. 0 0,1,2, 0,2. use -1 for CPU (default:\n",
            "                        0)\n",
            "  --checkpoints_dir CHECKPOINTS_DIR\n",
            "                        models are saved here (default: ./checkpoints)\n",
            "  --model MODEL         chooses which model to use. [cycle_gan | pix2pix |\n",
            "                        test | colorization] (default: cycle_gan)\n",
            "  --input_nc INPUT_NC   # of input image channels: 3 for RGB and 1 for\n",
            "                        grayscale (default: 3)\n",
            "  --output_nc OUTPUT_NC\n",
            "                        # of output image channels: 3 for RGB and 1 for\n",
            "                        grayscale (default: 3)\n",
            "  --ngf NGF             # of gen filters in the last conv layer (default: 64)\n",
            "  --ndf NDF             # of discrim filters in the first conv layer (default:\n",
            "                        64)\n",
            "  --netD NETD           specify discriminator architecture [basic | n_layers |\n",
            "                        pixel]. The basic model is a 70x70 PatchGAN. n_layers\n",
            "                        allows you to specify the layers in the discriminator\n",
            "                        (default: basic)\n",
            "  --netG NETG           specify generator architecture [resnet_9blocks |\n",
            "                        resnet_6blocks | unet_256 | unet_128] (default:\n",
            "                        resnet_9blocks)\n",
            "  --n_layers_D N_LAYERS_D\n",
            "                        only used if netD==n_layers (default: 3)\n",
            "  --norm NORM           instance normalization or batch normalization\n",
            "                        [instance | batch | none] (default: instance)\n",
            "  --init_type INIT_TYPE\n",
            "                        network initialization [normal | xavier | kaiming |\n",
            "                        orthogonal] (default: normal)\n",
            "  --init_gain INIT_GAIN\n",
            "                        scaling factor for normal, xavier and orthogonal.\n",
            "                        (default: 0.02)\n",
            "  --no_dropout          no dropout for the generator (default: False)\n",
            "  --dataset_mode DATASET_MODE\n",
            "                        chooses how datasets are loaded. [unaligned | aligned\n",
            "                        | single | colorization] (default: unaligned)\n",
            "  --direction DIRECTION\n",
            "                        AtoB or BtoA (default: AtoB)\n",
            "  --serial_batches      if true, takes images in order to make batches,\n",
            "                        otherwise takes them randomly (default: False)\n",
            "  --num_threads NUM_THREADS\n",
            "                        # threads for loading data (default: 4)\n",
            "  --batch_size BATCH_SIZE\n",
            "                        input batch size (default: 1)\n",
            "  --load_size LOAD_SIZE\n",
            "                        scale images to this size (default: 286)\n",
            "  --crop_size CROP_SIZE\n",
            "                        then crop to this size (default: 256)\n",
            "  --max_dataset_size MAX_DATASET_SIZE\n",
            "                        Maximum number of samples allowed per dataset. If the\n",
            "                        dataset directory contains more than max_dataset_size,\n",
            "                        only a subset is loaded. (default: inf)\n",
            "  --preprocess PREPROCESS\n",
            "                        scaling and cropping of images at load time\n",
            "                        [resize_and_crop | crop | scale_width |\n",
            "                        scale_width_and_crop | none] (default:\n",
            "                        resize_and_crop)\n",
            "  --no_flip             if specified, do not flip the images for data\n",
            "                        augmentation (default: False)\n",
            "  --display_winsize DISPLAY_WINSIZE\n",
            "                        display window size for both visdom and HTML (default:\n",
            "                        256)\n",
            "  --epoch EPOCH         which epoch to load? set to latest to use latest\n",
            "                        cached model (default: latest)\n",
            "  --load_iter LOAD_ITER\n",
            "                        which iteration to load? if load_iter > 0, the code\n",
            "                        will load models by iter_[load_iter]; otherwise, the\n",
            "                        code will load models by [epoch] (default: 0)\n",
            "  --verbose             if specified, print more debugging information\n",
            "                        (default: False)\n",
            "  --suffix SUFFIX       customized suffix: opt.name = opt.name + suffix: e.g.,\n",
            "                        {model}_{netG}_size{load_size} (default: )\n",
            "  --display_freq DISPLAY_FREQ\n",
            "                        frequency of showing training results on screen\n",
            "                        (default: 400)\n",
            "  --display_ncols DISPLAY_NCOLS\n",
            "                        if positive, display all images in a single visdom web\n",
            "                        panel with certain number of images per row. (default:\n",
            "                        4)\n",
            "  --display_id DISPLAY_ID\n",
            "                        window id of the web display (default: 1)\n",
            "  --display_server DISPLAY_SERVER\n",
            "                        visdom server of the web display (default:\n",
            "                        http://localhost)\n",
            "  --display_env DISPLAY_ENV\n",
            "                        visdom display environment name (default is \"main\")\n",
            "                        (default: main)\n",
            "  --display_port DISPLAY_PORT\n",
            "                        visdom port of the web display (default: 8097)\n",
            "  --update_html_freq UPDATE_HTML_FREQ\n",
            "                        frequency of saving training results to html (default:\n",
            "                        1000)\n",
            "  --print_freq PRINT_FREQ\n",
            "                        frequency of showing training results on console\n",
            "                        (default: 100)\n",
            "  --no_html             do not save intermediate training results to\n",
            "                        [opt.checkpoints_dir]/[opt.name]/web/ (default: False)\n",
            "  --save_latest_freq SAVE_LATEST_FREQ\n",
            "                        frequency of saving the latest results (default: 5000)\n",
            "  --save_epoch_freq SAVE_EPOCH_FREQ\n",
            "                        frequency of saving checkpoints at the end of epochs\n",
            "                        (default: 5)\n",
            "  --save_by_iter        whether saves model by iteration (default: False)\n",
            "  --continue_train      continue training: load the latest model (default:\n",
            "                        False)\n",
            "  --epoch_count EPOCH_COUNT\n",
            "                        the starting epoch count, we save the model by\n",
            "                        <epoch_count>, <epoch_count>+<save_latest_freq>, ...\n",
            "                        (default: 1)\n",
            "  --phase PHASE         train, val, test, etc (default: train)\n",
            "  --n_epochs N_EPOCHS   number of epochs with the initial learning rate\n",
            "                        (default: 100)\n",
            "  --n_epochs_decay N_EPOCHS_DECAY\n",
            "                        number of epochs to linearly decay learning rate to\n",
            "                        zero (default: 100)\n",
            "  --beta1 BETA1         momentum term of adam (default: 0.5)\n",
            "  --lr LR               initial learning rate for adam (default: 0.0002)\n",
            "  --gan_mode GAN_MODE   the type of GAN objective. [vanilla| lsgan | wgangp].\n",
            "                        vanilla GAN loss is the cross-entropy objective used\n",
            "                        in the original GAN paper. (default: lsgan)\n",
            "  --pool_size POOL_SIZE\n",
            "                        the size of image buffer that stores previously\n",
            "                        generated images (default: 50)\n",
            "  --lr_policy LR_POLICY\n",
            "                        learning rate policy. [linear | step | plateau |\n",
            "                        cosine] (default: linear)\n",
            "  --lr_decay_iters LR_DECAY_ITERS\n",
            "                        multiply by a gamma every lr_decay_iters iterations\n",
            "                        (default: 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --dataroot ./Doom_Health_Supreme_Split/ --name Doom_Health_Supreme_pix2pix --model pix2pix --input_nc 1 --output_nc 1 --netG unet_128 --load_size 128 --preprocess scale_width --gan_mode vanilla --gpu_ids 0 --dataset_mode aligned --crop_size 128 --print_freq 1000 --display_freq 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucHBm611VctE",
        "outputId": "f61232ea-777b-4ec8-ce9c-847d77ef717d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- Options ---------------\n",
            "               batch_size: 1                             \n",
            "                    beta1: 0.5                           \n",
            "          checkpoints_dir: ./checkpoints                 \n",
            "           continue_train: False                         \n",
            "                crop_size: 128                           \t[default: 256]\n",
            "                 dataroot: ./Doom_Health_Supreme_Split/  \t[default: None]\n",
            "             dataset_mode: aligned                       \n",
            "                direction: AtoB                          \n",
            "              display_env: main                          \n",
            "             display_freq: 1000                          \t[default: 400]\n",
            "               display_id: 1                             \n",
            "            display_ncols: 4                             \n",
            "             display_port: 8097                          \n",
            "           display_server: http://localhost              \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 1                             \n",
            "                 gan_mode: vanilla                       \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 1                             \t[default: 3]\n",
            "                  isTrain: True                          \t[default: None]\n",
            "                lambda_L1: 100.0                         \n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 128                           \t[default: 286]\n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: inf                           \n",
            "                    model: pix2pix                       \t[default: cycle_gan]\n",
            "                 n_epochs: 100                           \n",
            "           n_epochs_decay: 100                           \n",
            "               n_layers_D: 3                             \n",
            "                     name: Doom_Health_Supreme_pix2pix   \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: unet_128                      \t[default: unet_256]\n",
            "                      ngf: 64                            \n",
            "               no_dropout: False                         \n",
            "                  no_flip: False                         \n",
            "                  no_html: False                         \n",
            "                     norm: batch                         \n",
            "              num_threads: 4                             \n",
            "                output_nc: 1                             \t[default: 3]\n",
            "                    phase: train                         \n",
            "                pool_size: 0                             \n",
            "               preprocess: scale_width                   \t[default: resize_and_crop]\n",
            "               print_freq: 1000                          \t[default: 100]\n",
            "             save_by_iter: False                         \n",
            "          save_epoch_freq: 5                             \n",
            "         save_latest_freq: 5000                          \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "         update_html_freq: 1000                          \n",
            "                use_wandb: False                         \n",
            "                  verbose: False                         \n",
            "----------------- End -------------------\n",
            "dataset [AlignedDataset] was created\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "The number of training images = 2001\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "model [Pix2PixModel] was created\n",
            "---------- Networks initialized -------------\n",
            "[Network G] Total number of parameters : 41.823 M\n",
            "[Network D] Total number of parameters : 2.765 M\n",
            "-----------------------------------------------\n",
            "Setting up a new session...\n",
            "create web directory ./checkpoints/Doom_Health_Supreme_pix2pix/web...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 1, iters: 1000, time: 0.081, data: 0.163) G_GAN: 0.782 G_L1: 1.184 D_real: 0.507 D_fake: 0.757 \n",
            "(epoch: 1, iters: 2000, time: 0.075, data: 0.002) G_GAN: 0.680 G_L1: 1.160 D_real: 0.551 D_fake: 0.852 \n",
            "End of epoch 1 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 2, iters: 999, time: 0.078, data: 0.001) G_GAN: 0.688 G_L1: 0.885 D_real: 0.830 D_fake: 0.564 \n",
            "(epoch: 2, iters: 1999, time: 0.071, data: 0.002) G_GAN: 1.438 G_L1: 0.841 D_real: 0.367 D_fake: 0.798 \n",
            "End of epoch 2 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 3, iters: 998, time: 0.077, data: 0.001) G_GAN: 0.712 G_L1: 1.156 D_real: 0.726 D_fake: 0.638 \n",
            "saving the latest model (epoch 3, total_iters 5000)\n",
            "(epoch: 3, iters: 1998, time: 0.077, data: 0.005) G_GAN: 0.931 G_L1: 1.588 D_real: 0.851 D_fake: 0.424 \n",
            "End of epoch 3 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 4, iters: 997, time: 0.082, data: 0.001) G_GAN: 0.976 G_L1: 0.969 D_real: 0.668 D_fake: 0.538 \n",
            "(epoch: 4, iters: 1997, time: 0.074, data: 0.001) G_GAN: 1.293 G_L1: 1.222 D_real: 0.466 D_fake: 0.660 \n",
            "End of epoch 4 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 5, iters: 996, time: 0.081, data: 0.001) G_GAN: 0.857 G_L1: 0.817 D_real: 0.633 D_fake: 0.805 \n",
            "(epoch: 5, iters: 1996, time: 0.077, data: 0.002) G_GAN: 0.893 G_L1: 0.616 D_real: 0.650 D_fake: 0.728 \n",
            "saving the latest model (epoch 5, total_iters 10000)\n",
            "saving the model at the end of epoch 5, iters 10005\n",
            "End of epoch 5 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 6, iters: 995, time: 0.083, data: 0.001) G_GAN: 0.928 G_L1: 0.517 D_real: 0.266 D_fake: 1.078 \n",
            "(epoch: 6, iters: 1995, time: 0.077, data: 0.001) G_GAN: 1.068 G_L1: 0.759 D_real: 0.442 D_fake: 0.773 \n",
            "End of epoch 6 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 7, iters: 994, time: 0.085, data: 0.001) G_GAN: 1.294 G_L1: 0.651 D_real: 0.891 D_fake: 0.341 \n",
            "(epoch: 7, iters: 1994, time: 0.078, data: 0.001) G_GAN: 2.155 G_L1: 0.940 D_real: 0.065 D_fake: 0.602 \n",
            "End of epoch 7 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 8, iters: 993, time: 0.082, data: 0.001) G_GAN: 1.258 G_L1: 0.486 D_real: 0.324 D_fake: 0.591 \n",
            "saving the latest model (epoch 8, total_iters 15000)\n",
            "(epoch: 8, iters: 1993, time: 0.077, data: 0.003) G_GAN: 1.368 G_L1: 0.793 D_real: 0.840 D_fake: 0.751 \n",
            "End of epoch 8 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 9, iters: 992, time: 0.084, data: 0.001) G_GAN: 0.869 G_L1: 0.543 D_real: 0.958 D_fake: 0.530 \n",
            "(epoch: 9, iters: 1992, time: 0.071, data: 0.001) G_GAN: 1.906 G_L1: 0.559 D_real: 0.514 D_fake: 0.164 \n",
            "End of epoch 9 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 10, iters: 991, time: 0.084, data: 0.001) G_GAN: 1.632 G_L1: 0.398 D_real: 0.343 D_fake: 0.386 \n",
            "(epoch: 10, iters: 1991, time: 0.079, data: 0.001) G_GAN: 0.784 G_L1: 0.514 D_real: 0.859 D_fake: 0.407 \n",
            "saving the latest model (epoch 10, total_iters 20000)\n",
            "saving the model at the end of epoch 10, iters 20010\n",
            "End of epoch 10 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 11, iters: 990, time: 0.080, data: 0.006) G_GAN: 1.524 G_L1: 0.369 D_real: 0.230 D_fake: 0.711 \n",
            "(epoch: 11, iters: 1990, time: 0.081, data: 0.002) G_GAN: 0.964 G_L1: 0.717 D_real: 0.427 D_fake: 0.963 \n",
            "End of epoch 11 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 12, iters: 989, time: 0.089, data: 0.001) G_GAN: 0.780 G_L1: 0.432 D_real: 0.824 D_fake: 0.528 \n",
            "(epoch: 12, iters: 1989, time: 0.087, data: 0.004) G_GAN: 1.693 G_L1: 0.823 D_real: 0.353 D_fake: 1.039 \n",
            "End of epoch 12 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 13, iters: 988, time: 0.088, data: 0.004) G_GAN: 1.527 G_L1: 0.543 D_real: 0.684 D_fake: 0.278 \n",
            "saving the latest model (epoch 13, total_iters 25000)\n",
            "(epoch: 13, iters: 1988, time: 0.086, data: 0.002) G_GAN: 4.043 G_L1: 0.498 D_real: 0.131 D_fake: 0.044 \n",
            "End of epoch 13 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 14, iters: 987, time: 0.089, data: 0.002) G_GAN: 1.423 G_L1: 0.410 D_real: 0.563 D_fake: 0.502 \n",
            "(epoch: 14, iters: 1987, time: 0.162, data: 0.001) G_GAN: 2.529 G_L1: 0.484 D_real: 0.148 D_fake: 0.277 \n",
            "End of epoch 14 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 15, iters: 986, time: 0.090, data: 0.002) G_GAN: 1.548 G_L1: 0.474 D_real: 0.215 D_fake: 0.590 \n",
            "(epoch: 15, iters: 1986, time: 0.083, data: 0.001) G_GAN: 2.422 G_L1: 0.508 D_real: 0.032 D_fake: 0.849 \n",
            "saving the latest model (epoch 15, total_iters 30000)\n",
            "saving the model at the end of epoch 15, iters 30015\n",
            "End of epoch 15 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 16, iters: 985, time: 0.089, data: 0.003) G_GAN: 2.033 G_L1: 0.409 D_real: 0.091 D_fake: 1.139 \n",
            "(epoch: 16, iters: 1985, time: 0.084, data: 0.001) G_GAN: 1.990 G_L1: 0.521 D_real: 0.505 D_fake: 0.588 \n",
            "End of epoch 16 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 17, iters: 984, time: 0.090, data: 0.002) G_GAN: 1.581 G_L1: 0.388 D_real: 0.384 D_fake: 0.824 \n",
            "(epoch: 17, iters: 1984, time: 0.085, data: 0.001) G_GAN: 1.756 G_L1: 0.551 D_real: 0.587 D_fake: 0.868 \n",
            "End of epoch 17 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 18, iters: 983, time: 0.092, data: 0.001) G_GAN: 0.771 G_L1: 0.355 D_real: 0.610 D_fake: 0.774 \n",
            "saving the latest model (epoch 18, total_iters 35000)\n",
            "(epoch: 18, iters: 1983, time: 0.085, data: 0.002) G_GAN: 1.928 G_L1: 0.542 D_real: 0.253 D_fake: 0.936 \n",
            "End of epoch 18 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 19, iters: 982, time: 0.089, data: 0.001) G_GAN: 3.666 G_L1: 0.583 D_real: 0.128 D_fake: 0.041 \n",
            "(epoch: 19, iters: 1982, time: 0.086, data: 0.001) G_GAN: 1.292 G_L1: 0.701 D_real: 0.322 D_fake: 0.681 \n",
            "End of epoch 19 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 20, iters: 981, time: 0.094, data: 0.001) G_GAN: 2.647 G_L1: 0.405 D_real: 0.273 D_fake: 0.222 \n",
            "(epoch: 20, iters: 1981, time: 0.087, data: 0.001) G_GAN: 1.193 G_L1: 0.407 D_real: 0.535 D_fake: 0.498 \n",
            "saving the latest model (epoch 20, total_iters 40000)\n",
            "saving the model at the end of epoch 20, iters 40020\n",
            "End of epoch 20 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 21, iters: 980, time: 0.094, data: 0.002) G_GAN: 4.783 G_L1: 0.350 D_real: 0.293 D_fake: 0.479 \n",
            "(epoch: 21, iters: 1980, time: 0.089, data: 0.001) G_GAN: 3.371 G_L1: 0.704 D_real: 0.246 D_fake: 0.181 \n",
            "End of epoch 21 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 22, iters: 979, time: 0.107, data: 0.001) G_GAN: 1.053 G_L1: 0.612 D_real: 1.315 D_fake: 0.286 \n",
            "(epoch: 22, iters: 1979, time: 0.091, data: 0.003) G_GAN: 1.068 G_L1: 0.519 D_real: 0.695 D_fake: 0.583 \n",
            "End of epoch 22 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 23, iters: 978, time: 0.097, data: 0.001) G_GAN: 1.165 G_L1: 0.607 D_real: 0.825 D_fake: 0.634 \n",
            "saving the latest model (epoch 23, total_iters 45000)\n",
            "(epoch: 23, iters: 1978, time: 0.092, data: 0.002) G_GAN: 0.848 G_L1: 0.483 D_real: 1.178 D_fake: 0.155 \n",
            "End of epoch 23 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 24, iters: 977, time: 0.097, data: 0.001) G_GAN: 3.097 G_L1: 0.372 D_real: 0.172 D_fake: 0.201 \n",
            "(epoch: 24, iters: 1977, time: 0.091, data: 0.004) G_GAN: 5.300 G_L1: 0.277 D_real: 0.421 D_fake: 0.006 \n",
            "End of epoch 24 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 25, iters: 976, time: 0.097, data: 0.002) G_GAN: 1.328 G_L1: 0.517 D_real: 1.217 D_fake: 0.234 \n",
            "(epoch: 25, iters: 1976, time: 0.091, data: 0.001) G_GAN: 3.401 G_L1: 0.382 D_real: 0.127 D_fake: 0.133 \n",
            "saving the latest model (epoch 25, total_iters 50000)\n",
            "saving the model at the end of epoch 25, iters 50025\n",
            "End of epoch 25 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 26, iters: 975, time: 0.100, data: 0.008) G_GAN: 2.836 G_L1: 0.373 D_real: 0.137 D_fake: 0.624 \n",
            "(epoch: 26, iters: 1975, time: 0.095, data: 0.001) G_GAN: 1.022 G_L1: 0.321 D_real: 0.590 D_fake: 0.641 \n",
            "End of epoch 26 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 27, iters: 974, time: 0.098, data: 0.001) G_GAN: 0.948 G_L1: 0.440 D_real: 1.149 D_fake: 0.534 \n",
            "(epoch: 27, iters: 1974, time: 0.095, data: 0.001) G_GAN: 5.128 G_L1: 0.364 D_real: 0.029 D_fake: 1.346 \n",
            "End of epoch 27 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 28, iters: 973, time: 0.110, data: 0.001) G_GAN: 3.194 G_L1: 0.711 D_real: 0.060 D_fake: 0.631 \n",
            "saving the latest model (epoch 28, total_iters 55000)\n",
            "(epoch: 28, iters: 1973, time: 0.093, data: 0.005) G_GAN: 3.037 G_L1: 0.525 D_real: 0.261 D_fake: 0.636 \n",
            "End of epoch 28 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 29, iters: 972, time: 0.102, data: 0.002) G_GAN: 0.923 G_L1: 0.597 D_real: 0.882 D_fake: 0.327 \n",
            "(epoch: 29, iters: 1972, time: 0.095, data: 0.001) G_GAN: 1.331 G_L1: 0.531 D_real: 0.182 D_fake: 1.217 \n",
            "End of epoch 29 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 30, iters: 971, time: 0.092, data: 0.001) G_GAN: 1.055 G_L1: 0.537 D_real: 0.820 D_fake: 0.390 \n",
            "(epoch: 30, iters: 1971, time: 0.097, data: 0.002) G_GAN: 0.805 G_L1: 0.325 D_real: 0.919 D_fake: 0.478 \n",
            "saving the latest model (epoch 30, total_iters 60000)\n",
            "saving the model at the end of epoch 30, iters 60030\n",
            "End of epoch 30 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 31, iters: 970, time: 0.104, data: 0.006) G_GAN: 2.299 G_L1: 0.342 D_real: 0.317 D_fake: 0.813 \n",
            "(epoch: 31, iters: 1970, time: 0.100, data: 0.001) G_GAN: 2.115 G_L1: 0.410 D_real: 0.151 D_fake: 0.298 \n",
            "End of epoch 31 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 32, iters: 969, time: 0.107, data: 0.001) G_GAN: 1.225 G_L1: 0.580 D_real: 0.252 D_fake: 0.565 \n",
            "(epoch: 32, iters: 1969, time: 0.098, data: 0.005) G_GAN: 1.733 G_L1: 0.454 D_real: 0.420 D_fake: 0.916 \n",
            "End of epoch 32 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 33, iters: 968, time: 0.101, data: 0.001) G_GAN: 1.629 G_L1: 0.454 D_real: 0.473 D_fake: 0.765 \n",
            "saving the latest model (epoch 33, total_iters 65000)\n",
            "(epoch: 33, iters: 1968, time: 0.101, data: 0.002) G_GAN: 2.212 G_L1: 0.313 D_real: 0.242 D_fake: 0.422 \n",
            "End of epoch 33 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 34, iters: 967, time: 0.103, data: 0.001) G_GAN: 0.555 G_L1: 0.430 D_real: 0.633 D_fake: 0.187 \n",
            "(epoch: 34, iters: 1967, time: 0.191, data: 0.001) G_GAN: 0.977 G_L1: 0.438 D_real: 0.631 D_fake: 0.698 \n",
            "End of epoch 34 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 35, iters: 966, time: 0.116, data: 0.002) G_GAN: 2.417 G_L1: 0.632 D_real: 0.249 D_fake: 0.087 \n",
            "(epoch: 35, iters: 1966, time: 0.104, data: 0.002) G_GAN: 1.782 G_L1: 0.413 D_real: 0.310 D_fake: 0.599 \n",
            "saving the latest model (epoch 35, total_iters 70000)\n",
            "saving the model at the end of epoch 35, iters 70035\n",
            "End of epoch 35 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 36, iters: 965, time: 0.107, data: 0.005) G_GAN: 1.291 G_L1: 0.432 D_real: 0.417 D_fake: 0.513 \n",
            "(epoch: 36, iters: 1965, time: 0.102, data: 0.001) G_GAN: 1.503 G_L1: 0.294 D_real: 0.277 D_fake: 0.083 \n",
            "End of epoch 36 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 37, iters: 964, time: 0.109, data: 0.001) G_GAN: 1.091 G_L1: 0.450 D_real: 0.200 D_fake: 1.448 \n",
            "(epoch: 37, iters: 1964, time: 0.102, data: 0.001) G_GAN: 2.871 G_L1: 0.469 D_real: 0.139 D_fake: 0.433 \n",
            "End of epoch 37 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 38, iters: 963, time: 0.109, data: 0.004) G_GAN: 1.524 G_L1: 0.419 D_real: 0.396 D_fake: 0.828 \n",
            "saving the latest model (epoch 38, total_iters 75000)\n",
            "(epoch: 38, iters: 1963, time: 0.110, data: 0.002) G_GAN: 1.114 G_L1: 0.456 D_real: 1.238 D_fake: 0.554 \n",
            "End of epoch 38 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 39, iters: 962, time: 0.109, data: 0.002) G_GAN: 4.311 G_L1: 0.413 D_real: 0.187 D_fake: 0.666 \n",
            "(epoch: 39, iters: 1962, time: 0.105, data: 0.004) G_GAN: 2.414 G_L1: 0.324 D_real: 0.050 D_fake: 0.729 \n",
            "End of epoch 39 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 40, iters: 961, time: 0.113, data: 0.001) G_GAN: 0.871 G_L1: 0.517 D_real: 2.591 D_fake: 0.065 \n",
            "(epoch: 40, iters: 1961, time: 0.106, data: 0.001) G_GAN: 1.325 G_L1: 0.631 D_real: 0.085 D_fake: 2.069 \n",
            "saving the latest model (epoch 40, total_iters 80000)\n",
            "saving the model at the end of epoch 40, iters 80040\n",
            "End of epoch 40 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 41, iters: 960, time: 0.119, data: 0.002) G_GAN: 1.058 G_L1: 0.352 D_real: 0.622 D_fake: 0.606 \n",
            "(epoch: 41, iters: 1960, time: 0.106, data: 0.004) G_GAN: 1.573 G_L1: 0.403 D_real: 0.913 D_fake: 0.053 \n",
            "End of epoch 41 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 42, iters: 959, time: 0.112, data: 0.001) G_GAN: 1.566 G_L1: 0.565 D_real: 0.800 D_fake: 0.329 \n",
            "(epoch: 42, iters: 1959, time: 0.113, data: 0.001) G_GAN: 1.666 G_L1: 0.635 D_real: 1.056 D_fake: 0.425 \n",
            "End of epoch 42 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 43, iters: 958, time: 0.113, data: 0.001) G_GAN: 2.027 G_L1: 0.517 D_real: 0.645 D_fake: 0.230 \n",
            "saving the latest model (epoch 43, total_iters 85000)\n",
            "(epoch: 43, iters: 1958, time: 0.107, data: 0.002) G_GAN: 0.666 G_L1: 0.408 D_real: 1.543 D_fake: 0.401 \n",
            "End of epoch 43 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 44, iters: 957, time: 0.194, data: 0.001) G_GAN: 1.614 G_L1: 1.038 D_real: 0.418 D_fake: 0.106 \n",
            "(epoch: 44, iters: 1957, time: 0.107, data: 0.002) G_GAN: 1.936 G_L1: 0.576 D_real: 0.971 D_fake: 0.560 \n",
            "End of epoch 44 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 45, iters: 956, time: 0.116, data: 0.001) G_GAN: 1.324 G_L1: 0.460 D_real: 0.622 D_fake: 0.387 \n",
            "(epoch: 45, iters: 1956, time: 0.110, data: 0.002) G_GAN: 1.055 G_L1: 0.475 D_real: 2.099 D_fake: 0.139 \n",
            "saving the latest model (epoch 45, total_iters 90000)\n",
            "saving the model at the end of epoch 45, iters 90045\n",
            "End of epoch 45 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 46, iters: 955, time: 0.116, data: 0.002) G_GAN: 1.980 G_L1: 0.430 D_real: 0.590 D_fake: 0.081 \n",
            "(epoch: 46, iters: 1955, time: 0.111, data: 0.001) G_GAN: 1.150 G_L1: 0.364 D_real: 1.353 D_fake: 0.343 \n",
            "End of epoch 46 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 47, iters: 954, time: 0.125, data: 0.001) G_GAN: 2.663 G_L1: 0.645 D_real: 0.301 D_fake: 0.245 \n",
            "(epoch: 47, iters: 1954, time: 0.111, data: 0.002) G_GAN: 1.483 G_L1: 0.459 D_real: 0.784 D_fake: 0.420 \n",
            "End of epoch 47 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 48, iters: 953, time: 0.117, data: 0.001) G_GAN: 1.299 G_L1: 0.685 D_real: 0.326 D_fake: 0.972 \n",
            "saving the latest model (epoch 48, total_iters 95000)\n",
            "(epoch: 48, iters: 1953, time: 0.113, data: 0.002) G_GAN: 4.439 G_L1: 0.585 D_real: 0.168 D_fake: 0.022 \n",
            "End of epoch 48 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 49, iters: 952, time: 0.116, data: 0.010) G_GAN: 2.153 G_L1: 0.452 D_real: 0.269 D_fake: 0.481 \n",
            "(epoch: 49, iters: 1952, time: 0.111, data: 0.002) G_GAN: 1.255 G_L1: 0.600 D_real: 0.306 D_fake: 0.586 \n",
            "End of epoch 49 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 50, iters: 951, time: 0.120, data: 0.002) G_GAN: 2.111 G_L1: 0.753 D_real: 0.211 D_fake: 1.078 \n",
            "(epoch: 50, iters: 1951, time: 0.114, data: 0.002) G_GAN: 1.697 G_L1: 0.472 D_real: 0.336 D_fake: 0.286 \n",
            "saving the latest model (epoch 50, total_iters 100000)\n",
            "saving the model at the end of epoch 50, iters 100050\n",
            "End of epoch 50 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 51, iters: 950, time: 0.116, data: 0.002) G_GAN: 3.221 G_L1: 0.432 D_real: 0.019 D_fake: 0.188 \n",
            "(epoch: 51, iters: 1950, time: 0.209, data: 0.001) G_GAN: 2.974 G_L1: 0.468 D_real: 0.068 D_fake: 0.655 \n",
            "End of epoch 51 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 52, iters: 949, time: 0.138, data: 0.001) G_GAN: 3.255 G_L1: 0.364 D_real: 0.333 D_fake: 0.079 \n",
            "(epoch: 52, iters: 1949, time: 0.115, data: 0.001) G_GAN: 1.501 G_L1: 0.599 D_real: 0.666 D_fake: 0.231 \n",
            "End of epoch 52 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 53, iters: 948, time: 0.122, data: 0.002) G_GAN: 2.272 G_L1: 0.375 D_real: 1.099 D_fake: 0.175 \n",
            "saving the latest model (epoch 53, total_iters 105000)\n",
            "(epoch: 53, iters: 1948, time: 0.118, data: 0.007) G_GAN: 3.232 G_L1: 0.562 D_real: 0.228 D_fake: 0.084 \n",
            "End of epoch 53 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 54, iters: 947, time: 0.123, data: 0.002) G_GAN: 1.337 G_L1: 0.481 D_real: 0.383 D_fake: 0.650 \n",
            "(epoch: 54, iters: 1947, time: 0.118, data: 0.002) G_GAN: 4.472 G_L1: 0.377 D_real: 0.111 D_fake: 0.049 \n",
            "End of epoch 54 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 55, iters: 946, time: 0.121, data: 0.001) G_GAN: 3.941 G_L1: 0.456 D_real: 0.091 D_fake: 0.597 \n",
            "(epoch: 55, iters: 1946, time: 0.120, data: 0.001) G_GAN: 3.309 G_L1: 0.324 D_real: 0.371 D_fake: 0.063 \n",
            "saving the latest model (epoch 55, total_iters 110000)\n",
            "saving the model at the end of epoch 55, iters 110055\n",
            "End of epoch 55 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 56, iters: 945, time: 0.127, data: 0.004) G_GAN: 2.362 G_L1: 0.547 D_real: 0.201 D_fake: 0.354 \n",
            "(epoch: 56, iters: 1945, time: 0.115, data: 0.001) G_GAN: 5.392 G_L1: 0.397 D_real: 0.506 D_fake: 0.005 \n",
            "End of epoch 56 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 57, iters: 944, time: 0.126, data: 0.001) G_GAN: 2.668 G_L1: 0.571 D_real: 0.112 D_fake: 0.230 \n",
            "(epoch: 57, iters: 1944, time: 0.121, data: 0.006) G_GAN: 3.372 G_L1: 0.466 D_real: 0.028 D_fake: 0.312 \n",
            "End of epoch 57 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 58, iters: 943, time: 0.225, data: 0.001) G_GAN: 2.489 G_L1: 0.370 D_real: 0.194 D_fake: 0.790 \n",
            "saving the latest model (epoch 58, total_iters 115000)\n",
            "(epoch: 58, iters: 1943, time: 0.119, data: 0.005) G_GAN: 1.046 G_L1: 0.535 D_real: 0.579 D_fake: 0.610 \n",
            "End of epoch 58 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 59, iters: 942, time: 0.138, data: 0.001) G_GAN: 2.638 G_L1: 0.338 D_real: 0.247 D_fake: 0.550 \n",
            "(epoch: 59, iters: 1942, time: 0.123, data: 0.001) G_GAN: 1.942 G_L1: 0.349 D_real: 0.056 D_fake: 1.203 \n",
            "End of epoch 59 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 60, iters: 941, time: 0.129, data: 0.005) G_GAN: 1.505 G_L1: 0.459 D_real: 0.247 D_fake: 0.673 \n",
            "(epoch: 60, iters: 1941, time: 0.121, data: 0.001) G_GAN: 3.118 G_L1: 0.531 D_real: 0.048 D_fake: 0.322 \n",
            "saving the latest model (epoch 60, total_iters 120000)\n",
            "saving the model at the end of epoch 60, iters 120060\n",
            "End of epoch 60 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 61, iters: 940, time: 0.137, data: 0.002) G_GAN: 3.821 G_L1: 0.443 D_real: 0.139 D_fake: 0.111 \n",
            "(epoch: 61, iters: 1940, time: 0.129, data: 0.001) G_GAN: 1.350 G_L1: 0.369 D_real: 0.718 D_fake: 0.663 \n",
            "End of epoch 61 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 62, iters: 939, time: 0.132, data: 0.001) G_GAN: 1.003 G_L1: 0.389 D_real: 1.009 D_fake: 0.029 \n",
            "(epoch: 62, iters: 1939, time: 0.120, data: 0.002) G_GAN: 1.789 G_L1: 0.534 D_real: 0.042 D_fake: 0.743 \n",
            "End of epoch 62 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 63, iters: 938, time: 0.139, data: 0.001) G_GAN: 2.966 G_L1: 0.382 D_real: 0.121 D_fake: 0.332 \n",
            "saving the latest model (epoch 63, total_iters 125000)\n",
            "(epoch: 63, iters: 1938, time: 0.124, data: 0.006) G_GAN: 3.366 G_L1: 0.261 D_real: 0.194 D_fake: 0.255 \n",
            "End of epoch 63 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 64, iters: 937, time: 0.240, data: 0.005) G_GAN: 4.695 G_L1: 0.473 D_real: 0.046 D_fake: 0.765 \n",
            "(epoch: 64, iters: 1937, time: 0.122, data: 0.002) G_GAN: 2.383 G_L1: 0.514 D_real: 0.111 D_fake: 1.794 \n",
            "End of epoch 64 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 65, iters: 936, time: 0.150, data: 0.001) G_GAN: 1.405 G_L1: 0.510 D_real: 0.435 D_fake: 0.463 \n",
            "(epoch: 65, iters: 1936, time: 0.135, data: 0.006) G_GAN: 1.912 G_L1: 0.368 D_real: 0.786 D_fake: 0.149 \n",
            "saving the latest model (epoch 65, total_iters 130000)\n",
            "saving the model at the end of epoch 65, iters 130065\n",
            "End of epoch 65 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 66, iters: 935, time: 0.141, data: 0.002) G_GAN: 3.611 G_L1: 0.358 D_real: 0.222 D_fake: 0.069 \n",
            "(epoch: 66, iters: 1935, time: 0.130, data: 0.002) G_GAN: 1.477 G_L1: 0.364 D_real: 0.632 D_fake: 0.234 \n",
            "End of epoch 66 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 67, iters: 934, time: 0.140, data: 0.001) G_GAN: 2.042 G_L1: 0.344 D_real: 0.476 D_fake: 0.389 \n",
            "(epoch: 67, iters: 1934, time: 0.131, data: 0.005) G_GAN: 2.880 G_L1: 0.452 D_real: 0.107 D_fake: 0.417 \n",
            "End of epoch 67 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 68, iters: 933, time: 0.141, data: 0.001) G_GAN: 0.966 G_L1: 0.520 D_real: 1.721 D_fake: 0.013 \n",
            "saving the latest model (epoch 68, total_iters 135000)\n",
            "(epoch: 68, iters: 1933, time: 0.132, data: 0.003) G_GAN: 2.284 G_L1: 0.376 D_real: 0.072 D_fake: 0.592 \n",
            "End of epoch 68 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 69, iters: 932, time: 0.138, data: 0.001) G_GAN: 1.918 G_L1: 0.430 D_real: 0.356 D_fake: 0.172 \n",
            "(epoch: 69, iters: 1932, time: 0.248, data: 0.001) G_GAN: 1.782 G_L1: 0.404 D_real: 0.559 D_fake: 0.242 \n",
            "End of epoch 69 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 70, iters: 931, time: 0.138, data: 0.005) G_GAN: 4.939 G_L1: 0.660 D_real: 0.071 D_fake: 0.024 \n",
            "(epoch: 70, iters: 1931, time: 0.132, data: 0.001) G_GAN: 3.298 G_L1: 0.628 D_real: 0.158 D_fake: 0.236 \n",
            "saving the latest model (epoch 70, total_iters 140000)\n",
            "saving the model at the end of epoch 70, iters 140070\n",
            "End of epoch 70 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 71, iters: 930, time: 0.139, data: 0.002) G_GAN: 3.122 G_L1: 0.441 D_real: 0.244 D_fake: 0.390 \n",
            "(epoch: 71, iters: 1930, time: 0.134, data: 0.001) G_GAN: 1.945 G_L1: 0.465 D_real: 0.247 D_fake: 1.369 \n",
            "End of epoch 71 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 72, iters: 929, time: 0.142, data: 0.002) G_GAN: 2.840 G_L1: 0.883 D_real: 0.398 D_fake: 0.850 \n",
            "(epoch: 72, iters: 1929, time: 0.143, data: 0.002) G_GAN: 5.386 G_L1: 0.412 D_real: 0.121 D_fake: 0.020 \n",
            "End of epoch 72 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 73, iters: 928, time: 0.140, data: 0.002) G_GAN: 4.110 G_L1: 0.454 D_real: 0.049 D_fake: 0.208 \n",
            "saving the latest model (epoch 73, total_iters 145000)\n",
            "(epoch: 73, iters: 1928, time: 0.141, data: 0.002) G_GAN: 6.513 G_L1: 0.478 D_real: 0.032 D_fake: 0.009 \n",
            "End of epoch 73 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 74, iters: 927, time: 0.142, data: 0.001) G_GAN: 1.462 G_L1: 0.484 D_real: 0.729 D_fake: 0.198 \n",
            "(epoch: 74, iters: 1927, time: 0.243, data: 0.001) G_GAN: 3.920 G_L1: 0.378 D_real: 0.015 D_fake: 0.162 \n",
            "End of epoch 74 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 75, iters: 926, time: 0.145, data: 0.002) G_GAN: 2.345 G_L1: 0.323 D_real: 0.419 D_fake: 0.483 \n",
            "(epoch: 75, iters: 1926, time: 0.140, data: 0.002) G_GAN: 3.627 G_L1: 0.333 D_real: 0.413 D_fake: 0.036 \n",
            "saving the latest model (epoch 75, total_iters 150000)\n",
            "saving the model at the end of epoch 75, iters 150075\n",
            "End of epoch 75 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 76, iters: 925, time: 0.142, data: 0.002) G_GAN: 2.753 G_L1: 0.396 D_real: 0.165 D_fake: 0.717 \n",
            "(epoch: 76, iters: 1925, time: 0.139, data: 0.001) G_GAN: 2.764 G_L1: 0.492 D_real: 0.405 D_fake: 0.075 \n",
            "End of epoch 76 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 77, iters: 924, time: 0.147, data: 0.001) G_GAN: 3.374 G_L1: 0.486 D_real: 0.015 D_fake: 0.270 \n",
            "(epoch: 77, iters: 1924, time: 0.132, data: 0.002) G_GAN: 1.237 G_L1: 0.500 D_real: 0.443 D_fake: 0.964 \n",
            "End of epoch 77 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 78, iters: 923, time: 0.150, data: 0.003) G_GAN: 1.711 G_L1: 0.557 D_real: 0.583 D_fake: 0.255 \n",
            "saving the latest model (epoch 78, total_iters 155000)\n",
            "(epoch: 78, iters: 1923, time: 0.139, data: 0.002) G_GAN: 2.826 G_L1: 0.469 D_real: 0.404 D_fake: 0.111 \n",
            "End of epoch 78 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 79, iters: 922, time: 0.149, data: 0.002) G_GAN: 1.386 G_L1: 0.775 D_real: 0.101 D_fake: 1.567 \n",
            "(epoch: 79, iters: 1922, time: 0.268, data: 0.001) G_GAN: 2.292 G_L1: 0.363 D_real: 0.666 D_fake: 0.315 \n",
            "End of epoch 79 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 80, iters: 921, time: 0.143, data: 0.001) G_GAN: 3.022 G_L1: 0.522 D_real: 0.314 D_fake: 0.051 \n",
            "(epoch: 80, iters: 1921, time: 0.141, data: 0.002) G_GAN: 1.560 G_L1: 0.300 D_real: 0.665 D_fake: 0.492 \n",
            "saving the latest model (epoch 80, total_iters 160000)\n",
            "saving the model at the end of epoch 80, iters 160080\n",
            "End of epoch 80 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 81, iters: 920, time: 0.153, data: 0.002) G_GAN: 2.421 G_L1: 0.402 D_real: 0.875 D_fake: 0.035 \n",
            "(epoch: 81, iters: 1920, time: 0.149, data: 0.001) G_GAN: 2.258 G_L1: 0.482 D_real: 0.103 D_fake: 0.646 \n",
            "End of epoch 81 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 82, iters: 919, time: 0.150, data: 0.001) G_GAN: 2.119 G_L1: 0.419 D_real: 0.218 D_fake: 1.232 \n",
            "(epoch: 82, iters: 1919, time: 0.156, data: 0.003) G_GAN: 4.228 G_L1: 0.350 D_real: 0.187 D_fake: 0.031 \n",
            "End of epoch 82 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 83, iters: 918, time: 0.152, data: 0.002) G_GAN: 1.533 G_L1: 0.455 D_real: 0.554 D_fake: 0.290 \n",
            "saving the latest model (epoch 83, total_iters 165000)\n",
            "(epoch: 83, iters: 1918, time: 0.144, data: 0.002) G_GAN: 3.692 G_L1: 0.604 D_real: 0.042 D_fake: 0.043 \n",
            "End of epoch 83 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 84, iters: 917, time: 0.285, data: 0.002) G_GAN: 1.371 G_L1: 0.497 D_real: 0.414 D_fake: 0.392 \n",
            "(epoch: 84, iters: 1917, time: 0.138, data: 0.002) G_GAN: 1.303 G_L1: 0.297 D_real: 0.678 D_fake: 0.316 \n",
            "End of epoch 84 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 85, iters: 916, time: 0.150, data: 0.001) G_GAN: 2.739 G_L1: 0.420 D_real: 0.068 D_fake: 0.885 \n",
            "(epoch: 85, iters: 1916, time: 0.147, data: 0.001) G_GAN: 1.904 G_L1: 0.684 D_real: 0.628 D_fake: 0.126 \n",
            "saving the latest model (epoch 85, total_iters 170000)\n",
            "saving the model at the end of epoch 85, iters 170085\n",
            "End of epoch 85 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 86, iters: 915, time: 0.155, data: 0.002) G_GAN: 3.268 G_L1: 0.429 D_real: 0.723 D_fake: 0.053 \n",
            "(epoch: 86, iters: 1915, time: 0.146, data: 0.004) G_GAN: 0.996 G_L1: 0.329 D_real: 0.997 D_fake: 0.232 \n",
            "End of epoch 86 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 87, iters: 914, time: 0.159, data: 0.001) G_GAN: 3.138 G_L1: 0.313 D_real: 0.116 D_fake: 0.060 \n",
            "(epoch: 87, iters: 1914, time: 0.147, data: 0.002) G_GAN: 1.351 G_L1: 0.408 D_real: 1.660 D_fake: 0.092 \n",
            "End of epoch 87 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 88, iters: 913, time: 0.260, data: 0.001) G_GAN: 2.369 G_L1: 0.339 D_real: 0.221 D_fake: 0.530 \n",
            "saving the latest model (epoch 88, total_iters 175000)\n",
            "(epoch: 88, iters: 1913, time: 0.152, data: 0.004) G_GAN: 5.329 G_L1: 0.519 D_real: 0.190 D_fake: 0.014 \n",
            "End of epoch 88 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 89, iters: 912, time: 0.155, data: 0.002) G_GAN: 2.151 G_L1: 0.413 D_real: 0.438 D_fake: 0.623 \n",
            "(epoch: 89, iters: 1912, time: 0.149, data: 0.001) G_GAN: 1.176 G_L1: 0.450 D_real: 0.698 D_fake: 0.757 \n",
            "End of epoch 89 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 90, iters: 911, time: 0.154, data: 0.005) G_GAN: 2.601 G_L1: 0.363 D_real: 0.317 D_fake: 0.205 \n",
            "(epoch: 90, iters: 1911, time: 0.146, data: 0.001) G_GAN: 4.140 G_L1: 0.398 D_real: 0.067 D_fake: 0.114 \n",
            "saving the latest model (epoch 90, total_iters 180000)\n",
            "saving the model at the end of epoch 90, iters 180090\n",
            "End of epoch 90 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 91, iters: 910, time: 0.154, data: 0.003) G_GAN: 2.205 G_L1: 0.464 D_real: 0.204 D_fake: 0.962 \n",
            "(epoch: 91, iters: 1910, time: 0.150, data: 0.001) G_GAN: 4.571 G_L1: 0.394 D_real: 0.223 D_fake: 0.022 \n",
            "End of epoch 91 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 92, iters: 909, time: 0.270, data: 0.001) G_GAN: 2.523 G_L1: 0.308 D_real: 0.433 D_fake: 0.799 \n",
            "(epoch: 92, iters: 1909, time: 0.144, data: 0.001) G_GAN: 2.082 G_L1: 0.199 D_real: 0.054 D_fake: 1.117 \n",
            "End of epoch 92 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 93, iters: 908, time: 0.165, data: 0.002) G_GAN: 2.120 G_L1: 0.414 D_real: 0.072 D_fake: 0.380 \n",
            "saving the latest model (epoch 93, total_iters 185000)\n",
            "(epoch: 93, iters: 1908, time: 0.149, data: 0.004) G_GAN: 2.102 G_L1: 0.411 D_real: 0.117 D_fake: 0.481 \n",
            "End of epoch 93 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 94, iters: 907, time: 0.159, data: 0.005) G_GAN: 1.668 G_L1: 0.430 D_real: 1.072 D_fake: 0.057 \n",
            "(epoch: 94, iters: 1907, time: 0.153, data: 0.001) G_GAN: 3.479 G_L1: 0.484 D_real: 0.086 D_fake: 0.072 \n",
            "End of epoch 94 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 95, iters: 906, time: 0.162, data: 0.001) G_GAN: 1.075 G_L1: 0.419 D_real: 0.966 D_fake: 0.294 \n",
            "(epoch: 95, iters: 1906, time: 0.154, data: 0.002) G_GAN: 1.385 G_L1: 0.473 D_real: 0.570 D_fake: 0.764 \n",
            "saving the latest model (epoch 95, total_iters 190000)\n",
            "saving the model at the end of epoch 95, iters 190095\n",
            "End of epoch 95 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 96, iters: 905, time: 0.259, data: 0.002) G_GAN: 3.618 G_L1: 0.287 D_real: 0.076 D_fake: 0.082 \n",
            "(epoch: 96, iters: 1905, time: 0.154, data: 0.003) G_GAN: 3.696 G_L1: 0.416 D_real: 0.164 D_fake: 0.043 \n",
            "End of epoch 96 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 97, iters: 904, time: 0.169, data: 0.002) G_GAN: 1.301 G_L1: 0.313 D_real: 0.395 D_fake: 0.959 \n",
            "(epoch: 97, iters: 1904, time: 0.159, data: 0.001) G_GAN: 1.796 G_L1: 0.430 D_real: 0.454 D_fake: 0.967 \n",
            "End of epoch 97 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 98, iters: 903, time: 0.163, data: 0.001) G_GAN: 0.825 G_L1: 0.233 D_real: 1.323 D_fake: 0.403 \n",
            "saving the latest model (epoch 98, total_iters 195000)\n",
            "(epoch: 98, iters: 1903, time: 0.159, data: 0.010) G_GAN: 2.663 G_L1: 0.501 D_real: 0.372 D_fake: 0.093 \n",
            "End of epoch 98 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 99, iters: 902, time: 0.166, data: 0.005) G_GAN: 1.086 G_L1: 0.818 D_real: 1.135 D_fake: 0.329 \n",
            "(epoch: 99, iters: 1902, time: 0.269, data: 0.002) G_GAN: 2.302 G_L1: 0.412 D_real: 0.045 D_fake: 0.738 \n",
            "End of epoch 99 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0002000 -> 0.0001980\n",
            "(epoch: 100, iters: 901, time: 0.163, data: 0.006) G_GAN: 3.249 G_L1: 0.326 D_real: 0.020 D_fake: 0.919 \n",
            "(epoch: 100, iters: 1901, time: 0.159, data: 0.001) G_GAN: 0.786 G_L1: 0.232 D_real: 1.486 D_fake: 0.450 \n",
            "saving the latest model (epoch 100, total_iters 200000)\n",
            "saving the model at the end of epoch 100, iters 200100\n",
            "End of epoch 100 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0001980 -> 0.0001960\n",
            "(epoch: 101, iters: 900, time: 0.175, data: 0.002) G_GAN: 2.322 G_L1: 0.599 D_real: 0.319 D_fake: 0.268 \n",
            "(epoch: 101, iters: 1900, time: 0.160, data: 0.001) G_GAN: 1.532 G_L1: 0.412 D_real: 0.506 D_fake: 0.550 \n",
            "End of epoch 101 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001960 -> 0.0001941\n",
            "(epoch: 102, iters: 899, time: 0.170, data: 0.002) G_GAN: 1.810 G_L1: 0.365 D_real: 0.400 D_fake: 0.309 \n",
            "(epoch: 102, iters: 1899, time: 0.169, data: 0.002) G_GAN: 3.385 G_L1: 0.291 D_real: 0.129 D_fake: 0.236 \n",
            "End of epoch 102 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001941 -> 0.0001921\n",
            "(epoch: 103, iters: 898, time: 0.287, data: 0.002) G_GAN: 1.265 G_L1: 0.545 D_real: 0.424 D_fake: 0.694 \n",
            "saving the latest model (epoch 103, total_iters 205000)\n",
            "(epoch: 103, iters: 1898, time: 0.174, data: 0.004) G_GAN: 2.247 G_L1: 0.480 D_real: 0.252 D_fake: 0.820 \n",
            "End of epoch 103 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001921 -> 0.0001901\n",
            "(epoch: 104, iters: 897, time: 0.170, data: 0.002) G_GAN: 2.301 G_L1: 0.441 D_real: 0.274 D_fake: 0.140 \n",
            "(epoch: 104, iters: 1897, time: 0.170, data: 0.002) G_GAN: 1.439 G_L1: 0.645 D_real: 0.772 D_fake: 0.403 \n",
            "End of epoch 104 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001901 -> 0.0001881\n",
            "(epoch: 105, iters: 896, time: 0.178, data: 0.002) G_GAN: 1.963 G_L1: 0.443 D_real: 0.592 D_fake: 0.080 \n",
            "(epoch: 105, iters: 1896, time: 0.165, data: 0.002) G_GAN: 1.176 G_L1: 0.356 D_real: 1.241 D_fake: 0.161 \n",
            "saving the latest model (epoch 105, total_iters 210000)\n",
            "saving the model at the end of epoch 105, iters 210105\n",
            "End of epoch 105 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0001881 -> 0.0001861\n",
            "(epoch: 106, iters: 895, time: 0.172, data: 0.005) G_GAN: 2.045 G_L1: 0.654 D_real: 0.337 D_fake: 0.487 \n",
            "(epoch: 106, iters: 1895, time: 0.276, data: 0.002) G_GAN: 3.281 G_L1: 0.401 D_real: 0.012 D_fake: 0.195 \n",
            "End of epoch 106 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001861 -> 0.0001842\n",
            "(epoch: 107, iters: 894, time: 0.169, data: 0.002) G_GAN: 1.926 G_L1: 0.579 D_real: 0.666 D_fake: 0.388 \n",
            "(epoch: 107, iters: 1894, time: 0.171, data: 0.001) G_GAN: 3.686 G_L1: 0.388 D_real: 0.108 D_fake: 0.084 \n",
            "End of epoch 107 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001842 -> 0.0001822\n",
            "(epoch: 108, iters: 893, time: 0.176, data: 0.003) G_GAN: 3.293 G_L1: 0.354 D_real: 0.016 D_fake: 0.932 \n",
            "saving the latest model (epoch 108, total_iters 215000)\n",
            "(epoch: 108, iters: 1893, time: 0.165, data: 0.002) G_GAN: 2.417 G_L1: 0.421 D_real: 0.169 D_fake: 0.551 \n",
            "End of epoch 108 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001822 -> 0.0001802\n",
            "(epoch: 109, iters: 892, time: 0.174, data: 0.001) G_GAN: 1.937 G_L1: 0.474 D_real: 0.379 D_fake: 0.585 \n",
            "(epoch: 109, iters: 1892, time: 0.168, data: 0.002) G_GAN: 2.480 G_L1: 0.325 D_real: 0.321 D_fake: 0.177 \n",
            "End of epoch 109 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001802 -> 0.0001782\n",
            "(epoch: 110, iters: 891, time: 0.280, data: 0.001) G_GAN: 4.223 G_L1: 0.404 D_real: 0.311 D_fake: 0.013 \n",
            "(epoch: 110, iters: 1891, time: 0.162, data: 0.001) G_GAN: 2.191 G_L1: 0.589 D_real: 0.159 D_fake: 0.521 \n",
            "saving the latest model (epoch 110, total_iters 220000)\n",
            "saving the model at the end of epoch 110, iters 220110\n",
            "End of epoch 110 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0001782 -> 0.0001762\n",
            "(epoch: 111, iters: 890, time: 0.177, data: 0.002) G_GAN: 3.401 G_L1: 0.454 D_real: 0.117 D_fake: 0.113 \n",
            "(epoch: 111, iters: 1890, time: 0.183, data: 0.002) G_GAN: 2.367 G_L1: 0.450 D_real: 0.362 D_fake: 0.276 \n",
            "End of epoch 111 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001762 -> 0.0001743\n",
            "(epoch: 112, iters: 889, time: 0.182, data: 0.001) G_GAN: 4.856 G_L1: 0.485 D_real: 0.047 D_fake: 0.197 \n",
            "(epoch: 112, iters: 1889, time: 0.169, data: 0.002) G_GAN: 8.125 G_L1: 0.457 D_real: 0.042 D_fake: 0.004 \n",
            "End of epoch 112 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001743 -> 0.0001723\n",
            "(epoch: 113, iters: 888, time: 0.179, data: 0.001) G_GAN: 2.928 G_L1: 0.305 D_real: 0.021 D_fake: 0.861 \n",
            "saving the latest model (epoch 113, total_iters 225000)\n",
            "(epoch: 113, iters: 1888, time: 0.282, data: 0.002) G_GAN: 5.329 G_L1: 0.401 D_real: 0.033 D_fake: 0.019 \n",
            "End of epoch 113 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001723 -> 0.0001703\n",
            "(epoch: 114, iters: 887, time: 0.180, data: 0.001) G_GAN: 3.879 G_L1: 0.298 D_real: 0.125 D_fake: 0.092 \n",
            "(epoch: 114, iters: 1887, time: 0.177, data: 0.002) G_GAN: 1.196 G_L1: 0.398 D_real: 0.686 D_fake: 0.467 \n",
            "End of epoch 114 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001703 -> 0.0001683\n",
            "(epoch: 115, iters: 886, time: 0.180, data: 0.001) G_GAN: 0.874 G_L1: 0.275 D_real: 1.568 D_fake: 0.133 \n",
            "(epoch: 115, iters: 1886, time: 0.175, data: 0.001) G_GAN: 2.868 G_L1: 0.480 D_real: 0.286 D_fake: 0.389 \n",
            "saving the latest model (epoch 115, total_iters 230000)\n",
            "saving the model at the end of epoch 115, iters 230115\n",
            "End of epoch 115 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0001683 -> 0.0001663\n",
            "(epoch: 116, iters: 885, time: 0.175, data: 0.002) G_GAN: 2.710 G_L1: 0.226 D_real: 0.189 D_fake: 0.213 \n",
            "(epoch: 116, iters: 1885, time: 0.271, data: 0.001) G_GAN: 2.700 G_L1: 0.444 D_real: 0.096 D_fake: 0.303 \n",
            "End of epoch 116 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001663 -> 0.0001644\n",
            "(epoch: 117, iters: 884, time: 0.178, data: 0.001) G_GAN: 2.432 G_L1: 0.495 D_real: 0.311 D_fake: 0.588 \n",
            "(epoch: 117, iters: 1884, time: 0.180, data: 0.001) G_GAN: 2.619 G_L1: 0.398 D_real: 0.282 D_fake: 0.545 \n",
            "End of epoch 117 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001644 -> 0.0001624\n",
            "(epoch: 118, iters: 883, time: 0.187, data: 0.001) G_GAN: 6.143 G_L1: 0.268 D_real: 0.764 D_fake: 0.002 \n",
            "saving the latest model (epoch 118, total_iters 235000)\n",
            "(epoch: 118, iters: 1883, time: 0.178, data: 0.002) G_GAN: 1.961 G_L1: 0.917 D_real: 0.647 D_fake: 0.040 \n",
            "End of epoch 118 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001624 -> 0.0001604\n",
            "(epoch: 119, iters: 882, time: 0.182, data: 0.005) G_GAN: 3.303 G_L1: 0.699 D_real: 0.047 D_fake: 0.437 \n",
            "(epoch: 119, iters: 1882, time: 0.287, data: 0.002) G_GAN: 5.420 G_L1: 0.493 D_real: 0.325 D_fake: 0.013 \n",
            "End of epoch 119 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001604 -> 0.0001584\n",
            "(epoch: 120, iters: 881, time: 0.186, data: 0.001) G_GAN: 2.182 G_L1: 0.438 D_real: 0.126 D_fake: 0.245 \n",
            "(epoch: 120, iters: 1881, time: 0.179, data: 0.002) G_GAN: 2.258 G_L1: 0.356 D_real: 0.177 D_fake: 0.831 \n",
            "saving the latest model (epoch 120, total_iters 240000)\n",
            "saving the model at the end of epoch 120, iters 240120\n",
            "End of epoch 120 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0001584 -> 0.0001564\n",
            "(epoch: 121, iters: 880, time: 0.184, data: 0.002) G_GAN: 3.617 G_L1: 0.567 D_real: 0.090 D_fake: 0.160 \n",
            "(epoch: 121, iters: 1880, time: 0.178, data: 0.003) G_GAN: 4.365 G_L1: 0.369 D_real: 0.174 D_fake: 0.024 \n",
            "End of epoch 121 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001564 -> 0.0001545\n",
            "(epoch: 122, iters: 879, time: 0.195, data: 0.001) G_GAN: 2.285 G_L1: 0.243 D_real: 0.673 D_fake: 0.125 \n",
            "(epoch: 122, iters: 1879, time: 0.302, data: 0.001) G_GAN: 1.824 G_L1: 0.341 D_real: 0.240 D_fake: 0.254 \n",
            "End of epoch 122 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001545 -> 0.0001525\n",
            "(epoch: 123, iters: 878, time: 0.185, data: 0.003) G_GAN: 1.910 G_L1: 0.493 D_real: 0.209 D_fake: 0.372 \n",
            "saving the latest model (epoch 123, total_iters 245000)\n",
            "(epoch: 123, iters: 1878, time: 0.182, data: 0.009) G_GAN: 1.859 G_L1: 0.369 D_real: 0.349 D_fake: 0.870 \n",
            "End of epoch 123 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001525 -> 0.0001505\n",
            "(epoch: 124, iters: 877, time: 0.202, data: 0.002) G_GAN: 2.160 G_L1: 0.396 D_real: 0.438 D_fake: 0.641 \n",
            "(epoch: 124, iters: 1877, time: 0.180, data: 0.002) G_GAN: 2.548 G_L1: 0.579 D_real: 0.299 D_fake: 0.199 \n",
            "End of epoch 124 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001505 -> 0.0001485\n",
            "(epoch: 125, iters: 876, time: 0.203, data: 0.002) G_GAN: 2.145 G_L1: 0.437 D_real: 0.294 D_fake: 0.220 \n",
            "(epoch: 125, iters: 1876, time: 0.288, data: 0.004) G_GAN: 3.976 G_L1: 0.393 D_real: 0.276 D_fake: 0.037 \n",
            "saving the latest model (epoch 125, total_iters 250000)\n",
            "saving the model at the end of epoch 125, iters 250125\n",
            "End of epoch 125 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0001485 -> 0.0001465\n",
            "(epoch: 126, iters: 875, time: 0.200, data: 0.002) G_GAN: 2.474 G_L1: 0.370 D_real: 0.308 D_fake: 0.360 \n",
            "(epoch: 126, iters: 1875, time: 0.184, data: 0.002) G_GAN: 2.751 G_L1: 0.639 D_real: 0.110 D_fake: 0.599 \n",
            "End of epoch 126 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001465 -> 0.0001446\n",
            "(epoch: 127, iters: 874, time: 0.212, data: 0.001) G_GAN: 3.739 G_L1: 0.321 D_real: 0.069 D_fake: 0.468 \n",
            "(epoch: 127, iters: 1874, time: 0.188, data: 0.001) G_GAN: 3.167 G_L1: 0.371 D_real: 0.191 D_fake: 0.108 \n",
            "End of epoch 127 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001446 -> 0.0001426\n",
            "(epoch: 128, iters: 873, time: 0.192, data: 0.002) G_GAN: 2.424 G_L1: 0.374 D_real: 0.239 D_fake: 0.275 \n",
            "saving the latest model (epoch 128, total_iters 255000)\n",
            "(epoch: 128, iters: 1873, time: 0.294, data: 0.002) G_GAN: 3.645 G_L1: 0.275 D_real: 1.523 D_fake: 0.015 \n",
            "End of epoch 128 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001426 -> 0.0001406\n",
            "(epoch: 129, iters: 872, time: 0.190, data: 0.003) G_GAN: 3.109 G_L1: 0.412 D_real: 0.240 D_fake: 0.177 \n",
            "(epoch: 129, iters: 1872, time: 0.202, data: 0.001) G_GAN: 4.759 G_L1: 0.474 D_real: 0.097 D_fake: 0.022 \n",
            "End of epoch 129 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001406 -> 0.0001386\n",
            "(epoch: 130, iters: 871, time: 0.201, data: 0.002) G_GAN: 2.295 G_L1: 0.224 D_real: 1.692 D_fake: 0.043 \n",
            "(epoch: 130, iters: 1871, time: 0.193, data: 0.002) G_GAN: 1.412 G_L1: 0.259 D_real: 1.333 D_fake: 0.154 \n",
            "saving the latest model (epoch 130, total_iters 260000)\n",
            "saving the model at the end of epoch 130, iters 260130\n",
            "End of epoch 130 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0001386 -> 0.0001366\n",
            "(epoch: 131, iters: 870, time: 0.312, data: 0.008) G_GAN: 2.269 G_L1: 0.373 D_real: 0.379 D_fake: 0.373 \n",
            "(epoch: 131, iters: 1870, time: 0.179, data: 0.001) G_GAN: 1.419 G_L1: 0.397 D_real: 0.786 D_fake: 0.284 \n",
            "End of epoch 131 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001366 -> 0.0001347\n",
            "(epoch: 132, iters: 869, time: 0.197, data: 0.001) G_GAN: 4.335 G_L1: 0.428 D_real: 0.093 D_fake: 0.079 \n",
            "(epoch: 132, iters: 1869, time: 0.196, data: 0.002) G_GAN: 5.207 G_L1: 0.395 D_real: 0.349 D_fake: 0.017 \n",
            "End of epoch 132 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001347 -> 0.0001327\n",
            "(epoch: 133, iters: 868, time: 0.195, data: 0.001) G_GAN: 7.398 G_L1: 0.427 D_real: 0.072 D_fake: 0.002 \n",
            "saving the latest model (epoch 133, total_iters 265000)\n",
            "(epoch: 133, iters: 1868, time: 0.189, data: 0.002) G_GAN: 2.213 G_L1: 0.329 D_real: 0.311 D_fake: 0.640 \n",
            "End of epoch 133 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001327 -> 0.0001307\n",
            "(epoch: 134, iters: 867, time: 0.292, data: 0.002) G_GAN: 3.199 G_L1: 0.327 D_real: 0.046 D_fake: 0.694 \n",
            "(epoch: 134, iters: 1867, time: 0.184, data: 0.004) G_GAN: 1.936 G_L1: 0.437 D_real: 0.269 D_fake: 0.627 \n",
            "End of epoch 134 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001307 -> 0.0001287\n",
            "(epoch: 135, iters: 866, time: 0.205, data: 0.002) G_GAN: 2.049 G_L1: 0.460 D_real: 0.607 D_fake: 0.628 \n",
            "(epoch: 135, iters: 1866, time: 0.191, data: 0.001) G_GAN: 2.122 G_L1: 0.670 D_real: 0.137 D_fake: 0.340 \n",
            "saving the latest model (epoch 135, total_iters 270000)\n",
            "saving the model at the end of epoch 135, iters 270135\n",
            "End of epoch 135 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0001287 -> 0.0001267\n",
            "(epoch: 136, iters: 865, time: 0.204, data: 0.005) G_GAN: 8.255 G_L1: 0.383 D_real: 0.017 D_fake: 0.001 \n",
            "(epoch: 136, iters: 1865, time: 0.294, data: 0.002) G_GAN: 3.708 G_L1: 0.270 D_real: 0.067 D_fake: 0.817 \n",
            "End of epoch 136 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001267 -> 0.0001248\n",
            "(epoch: 137, iters: 864, time: 0.197, data: 0.002) G_GAN: 3.354 G_L1: 0.348 D_real: 0.272 D_fake: 0.224 \n",
            "(epoch: 137, iters: 1864, time: 0.193, data: 0.002) G_GAN: 2.302 G_L1: 0.577 D_real: 0.269 D_fake: 0.302 \n",
            "End of epoch 137 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001248 -> 0.0001228\n",
            "(epoch: 138, iters: 863, time: 0.209, data: 0.001) G_GAN: 1.086 G_L1: 0.373 D_real: 1.171 D_fake: 0.256 \n",
            "saving the latest model (epoch 138, total_iters 275000)\n",
            "(epoch: 138, iters: 1863, time: 0.195, data: 0.002) G_GAN: 2.321 G_L1: 0.459 D_real: 0.664 D_fake: 0.415 \n",
            "End of epoch 138 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001228 -> 0.0001208\n",
            "(epoch: 139, iters: 862, time: 0.201, data: 0.004) G_GAN: 6.109 G_L1: 0.327 D_real: 0.190 D_fake: 0.006 \n",
            "(epoch: 139, iters: 1862, time: 0.321, data: 0.002) G_GAN: 4.050 G_L1: 0.264 D_real: 0.172 D_fake: 0.019 \n",
            "End of epoch 139 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001208 -> 0.0001188\n",
            "(epoch: 140, iters: 861, time: 0.215, data: 0.002) G_GAN: 2.967 G_L1: 0.177 D_real: 0.638 D_fake: 0.041 \n",
            "(epoch: 140, iters: 1861, time: 0.194, data: 0.002) G_GAN: 4.248 G_L1: 0.365 D_real: 0.027 D_fake: 0.128 \n",
            "saving the latest model (epoch 140, total_iters 280000)\n",
            "saving the model at the end of epoch 140, iters 280140\n",
            "End of epoch 140 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0001188 -> 0.0001168\n",
            "(epoch: 141, iters: 860, time: 0.212, data: 0.002) G_GAN: 2.759 G_L1: 0.506 D_real: 0.471 D_fake: 0.154 \n",
            "(epoch: 141, iters: 1860, time: 0.198, data: 0.003) G_GAN: 1.744 G_L1: 0.295 D_real: 0.274 D_fake: 0.555 \n",
            "End of epoch 141 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001168 -> 0.0001149\n",
            "(epoch: 142, iters: 859, time: 0.308, data: 0.001) G_GAN: 2.388 G_L1: 0.488 D_real: 0.404 D_fake: 0.308 \n",
            "(epoch: 142, iters: 1859, time: 0.191, data: 0.003) G_GAN: 2.698 G_L1: 0.390 D_real: 0.669 D_fake: 0.134 \n",
            "End of epoch 142 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001149 -> 0.0001129\n",
            "(epoch: 143, iters: 858, time: 0.214, data: 0.002) G_GAN: 3.099 G_L1: 0.351 D_real: 0.017 D_fake: 0.270 \n",
            "saving the latest model (epoch 143, total_iters 285000)\n",
            "(epoch: 143, iters: 1858, time: 0.196, data: 0.002) G_GAN: 3.683 G_L1: 0.550 D_real: 0.111 D_fake: 0.108 \n",
            "End of epoch 143 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001129 -> 0.0001109\n",
            "(epoch: 144, iters: 857, time: 0.209, data: 0.002) G_GAN: 3.575 G_L1: 0.990 D_real: 0.121 D_fake: 0.737 \n",
            "(epoch: 144, iters: 1857, time: 0.304, data: 0.002) G_GAN: 3.953 G_L1: 0.500 D_real: 0.038 D_fake: 0.176 \n",
            "End of epoch 144 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001109 -> 0.0001089\n",
            "(epoch: 145, iters: 856, time: 0.218, data: 0.002) G_GAN: 2.642 G_L1: 0.590 D_real: 0.147 D_fake: 0.607 \n",
            "(epoch: 145, iters: 1856, time: 0.200, data: 0.001) G_GAN: 5.087 G_L1: 0.408 D_real: 0.184 D_fake: 0.016 \n",
            "saving the latest model (epoch 145, total_iters 290000)\n",
            "saving the model at the end of epoch 145, iters 290145\n",
            "End of epoch 145 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0001089 -> 0.0001069\n",
            "(epoch: 146, iters: 855, time: 0.220, data: 0.002) G_GAN: 2.005 G_L1: 0.498 D_real: 0.177 D_fake: 0.717 \n",
            "(epoch: 146, iters: 1855, time: 0.199, data: 0.002) G_GAN: 3.385 G_L1: 0.442 D_real: 0.132 D_fake: 0.142 \n",
            "End of epoch 146 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001069 -> 0.0001050\n",
            "(epoch: 147, iters: 854, time: 0.309, data: 0.002) G_GAN: 3.356 G_L1: 0.288 D_real: 0.094 D_fake: 0.108 \n",
            "(epoch: 147, iters: 1854, time: 0.192, data: 0.003) G_GAN: 6.589 G_L1: 0.341 D_real: 0.194 D_fake: 0.004 \n",
            "End of epoch 147 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001050 -> 0.0001030\n",
            "(epoch: 148, iters: 853, time: 0.209, data: 0.005) G_GAN: 3.845 G_L1: 0.599 D_real: 0.017 D_fake: 0.101 \n",
            "saving the latest model (epoch 148, total_iters 295000)\n",
            "(epoch: 148, iters: 1853, time: 0.203, data: 0.002) G_GAN: 3.910 G_L1: 0.346 D_real: 0.157 D_fake: 0.048 \n",
            "End of epoch 148 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0001030 -> 0.0001010\n",
            "(epoch: 149, iters: 852, time: 0.231, data: 0.001) G_GAN: 2.456 G_L1: 0.480 D_real: 0.333 D_fake: 0.186 \n",
            "(epoch: 149, iters: 1852, time: 0.313, data: 0.002) G_GAN: 2.051 G_L1: 0.474 D_real: 0.377 D_fake: 0.588 \n",
            "End of epoch 149 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0001010 -> 0.0000990\n",
            "(epoch: 150, iters: 851, time: 0.211, data: 0.002) G_GAN: 4.417 G_L1: 0.771 D_real: 0.045 D_fake: 0.040 \n",
            "(epoch: 150, iters: 1851, time: 0.203, data: 0.001) G_GAN: 2.649 G_L1: 0.286 D_real: 0.080 D_fake: 0.304 \n",
            "saving the latest model (epoch 150, total_iters 300000)\n",
            "saving the model at the end of epoch 150, iters 300150\n",
            "End of epoch 150 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0000990 -> 0.0000970\n",
            "(epoch: 151, iters: 850, time: 0.221, data: 0.005) G_GAN: 2.877 G_L1: 0.200 D_real: 0.316 D_fake: 0.054 \n",
            "(epoch: 151, iters: 1850, time: 0.209, data: 0.002) G_GAN: 1.923 G_L1: 0.560 D_real: 0.320 D_fake: 0.831 \n",
            "End of epoch 151 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000970 -> 0.0000950\n",
            "(epoch: 152, iters: 849, time: 0.318, data: 0.001) G_GAN: 1.687 G_L1: 0.251 D_real: 0.602 D_fake: 0.209 \n",
            "(epoch: 152, iters: 1849, time: 0.198, data: 0.002) G_GAN: 5.195 G_L1: 0.505 D_real: 0.015 D_fake: 0.023 \n",
            "End of epoch 152 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000950 -> 0.0000931\n",
            "(epoch: 153, iters: 848, time: 0.222, data: 0.004) G_GAN: 3.961 G_L1: 0.570 D_real: 0.108 D_fake: 0.061 \n",
            "saving the latest model (epoch 153, total_iters 305000)\n",
            "(epoch: 153, iters: 1848, time: 0.219, data: 0.002) G_GAN: 2.834 G_L1: 0.356 D_real: 0.120 D_fake: 0.149 \n",
            "End of epoch 153 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0000931 -> 0.0000911\n",
            "(epoch: 154, iters: 847, time: 0.342, data: 0.002) G_GAN: 2.243 G_L1: 0.536 D_real: 0.181 D_fake: 0.679 \n",
            "(epoch: 154, iters: 1847, time: 0.203, data: 0.003) G_GAN: 4.540 G_L1: 0.363 D_real: 0.015 D_fake: 0.098 \n",
            "End of epoch 154 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000911 -> 0.0000891\n",
            "(epoch: 155, iters: 846, time: 0.230, data: 0.002) G_GAN: 6.099 G_L1: 0.516 D_real: 0.023 D_fake: 0.009 \n",
            "(epoch: 155, iters: 1846, time: 0.211, data: 0.002) G_GAN: 2.623 G_L1: 0.228 D_real: 0.323 D_fake: 0.137 \n",
            "saving the latest model (epoch 155, total_iters 310000)\n",
            "saving the model at the end of epoch 155, iters 310155\n",
            "End of epoch 155 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0000891 -> 0.0000871\n",
            "(epoch: 156, iters: 845, time: 0.225, data: 0.002) G_GAN: 3.134 G_L1: 0.391 D_real: 0.010 D_fake: 0.144 \n",
            "(epoch: 156, iters: 1845, time: 0.310, data: 0.002) G_GAN: 2.902 G_L1: 0.300 D_real: 0.262 D_fake: 0.207 \n",
            "End of epoch 156 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000871 -> 0.0000851\n",
            "(epoch: 157, iters: 844, time: 0.232, data: 0.001) G_GAN: 3.012 G_L1: 0.452 D_real: 0.117 D_fake: 0.189 \n",
            "(epoch: 157, iters: 1844, time: 0.209, data: 0.002) G_GAN: 2.081 G_L1: 0.202 D_real: 0.522 D_fake: 0.160 \n",
            "End of epoch 157 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000851 -> 0.0000832\n",
            "(epoch: 158, iters: 843, time: 0.228, data: 0.002) G_GAN: 1.659 G_L1: 0.251 D_real: 0.746 D_fake: 0.309 \n",
            "saving the latest model (epoch 158, total_iters 315000)\n",
            "(epoch: 158, iters: 1843, time: 0.216, data: 0.002) G_GAN: 3.087 G_L1: 0.459 D_real: 0.146 D_fake: 0.097 \n",
            "End of epoch 158 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0000832 -> 0.0000812\n",
            "(epoch: 159, iters: 842, time: 0.333, data: 0.002) G_GAN: 3.899 G_L1: 0.512 D_real: 0.099 D_fake: 0.088 \n",
            "(epoch: 159, iters: 1842, time: 0.205, data: 0.001) G_GAN: 3.262 G_L1: 0.445 D_real: 0.184 D_fake: 0.280 \n",
            "End of epoch 159 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000812 -> 0.0000792\n",
            "(epoch: 160, iters: 841, time: 0.228, data: 0.002) G_GAN: 3.958 G_L1: 0.880 D_real: 0.006 D_fake: 0.731 \n",
            "(epoch: 160, iters: 1841, time: 0.223, data: 0.001) G_GAN: 0.965 G_L1: 0.342 D_real: 1.201 D_fake: 0.087 \n",
            "saving the latest model (epoch 160, total_iters 320000)\n",
            "saving the model at the end of epoch 160, iters 320160\n",
            "End of epoch 160 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0000792 -> 0.0000772\n",
            "(epoch: 161, iters: 840, time: 0.344, data: 0.006) G_GAN: 2.535 G_L1: 0.582 D_real: 0.278 D_fake: 0.310 \n",
            "(epoch: 161, iters: 1840, time: 0.208, data: 0.004) G_GAN: 2.822 G_L1: 0.595 D_real: 0.073 D_fake: 0.420 \n",
            "End of epoch 161 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000772 -> 0.0000752\n",
            "(epoch: 162, iters: 839, time: 0.230, data: 0.002) G_GAN: 1.626 G_L1: 0.383 D_real: 0.195 D_fake: 1.127 \n",
            "(epoch: 162, iters: 1839, time: 0.213, data: 0.001) G_GAN: 1.581 G_L1: 0.345 D_real: 0.951 D_fake: 0.099 \n",
            "End of epoch 162 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000752 -> 0.0000733\n",
            "(epoch: 163, iters: 838, time: 0.224, data: 0.003) G_GAN: 2.246 G_L1: 0.422 D_real: 0.805 D_fake: 0.169 \n",
            "saving the latest model (epoch 163, total_iters 325000)\n",
            "(epoch: 163, iters: 1838, time: 0.317, data: 0.007) G_GAN: 4.339 G_L1: 0.473 D_real: 0.001 D_fake: 0.125 \n",
            "End of epoch 163 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0000733 -> 0.0000713\n",
            "(epoch: 164, iters: 837, time: 0.220, data: 0.002) G_GAN: 1.778 G_L1: 0.370 D_real: 0.123 D_fake: 0.742 \n",
            "(epoch: 164, iters: 1837, time: 0.220, data: 0.001) G_GAN: 3.012 G_L1: 0.466 D_real: 0.020 D_fake: 0.458 \n",
            "End of epoch 164 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000713 -> 0.0000693\n",
            "(epoch: 165, iters: 836, time: 0.225, data: 0.002) G_GAN: 2.804 G_L1: 0.434 D_real: 0.067 D_fake: 0.338 \n",
            "(epoch: 165, iters: 1836, time: 0.316, data: 0.002) G_GAN: 2.426 G_L1: 0.346 D_real: 0.243 D_fake: 0.392 \n",
            "saving the latest model (epoch 165, total_iters 330000)\n",
            "saving the model at the end of epoch 165, iters 330165\n",
            "End of epoch 165 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0000693 -> 0.0000673\n",
            "(epoch: 166, iters: 835, time: 0.227, data: 0.003) G_GAN: 1.905 G_L1: 0.470 D_real: 0.078 D_fake: 0.719 \n",
            "(epoch: 166, iters: 1835, time: 0.224, data: 0.001) G_GAN: 2.392 G_L1: 0.383 D_real: 0.576 D_fake: 0.381 \n",
            "End of epoch 166 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000673 -> 0.0000653\n",
            "(epoch: 167, iters: 834, time: 0.226, data: 0.002) G_GAN: 4.104 G_L1: 0.207 D_real: 0.731 D_fake: 0.021 \n",
            "(epoch: 167, iters: 1834, time: 0.231, data: 0.002) G_GAN: 2.262 G_L1: 0.359 D_real: 0.568 D_fake: 0.254 \n",
            "End of epoch 167 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000653 -> 0.0000634\n",
            "(epoch: 168, iters: 833, time: 0.333, data: 0.002) G_GAN: 2.231 G_L1: 0.317 D_real: 0.345 D_fake: 0.161 \n",
            "saving the latest model (epoch 168, total_iters 335000)\n",
            "(epoch: 168, iters: 1833, time: 0.222, data: 0.018) G_GAN: 2.127 G_L1: 0.547 D_real: 0.137 D_fake: 0.709 \n",
            "End of epoch 168 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0000634 -> 0.0000614\n",
            "(epoch: 169, iters: 832, time: 0.238, data: 0.002) G_GAN: 4.440 G_L1: 0.282 D_real: 0.062 D_fake: 0.054 \n",
            "(epoch: 169, iters: 1832, time: 0.220, data: 0.002) G_GAN: 2.819 G_L1: 0.208 D_real: 0.260 D_fake: 0.149 \n",
            "End of epoch 169 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000614 -> 0.0000594\n",
            "(epoch: 170, iters: 831, time: 0.377, data: 0.001) G_GAN: 2.614 G_L1: 0.356 D_real: 0.131 D_fake: 0.362 \n",
            "(epoch: 170, iters: 1831, time: 0.215, data: 0.005) G_GAN: 2.347 G_L1: 0.484 D_real: 0.062 D_fake: 0.287 \n",
            "saving the latest model (epoch 170, total_iters 340000)\n",
            "saving the model at the end of epoch 170, iters 340170\n",
            "End of epoch 170 / 200 \t Time Taken: 68 sec\n",
            "learning rate 0.0000594 -> 0.0000574\n",
            "(epoch: 171, iters: 830, time: 0.225, data: 0.003) G_GAN: 3.342 G_L1: 0.397 D_real: 0.009 D_fake: 0.440 \n",
            "(epoch: 171, iters: 1830, time: 0.227, data: 0.001) G_GAN: 3.091 G_L1: 0.273 D_real: 0.136 D_fake: 0.307 \n",
            "End of epoch 171 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000574 -> 0.0000554\n",
            "(epoch: 172, iters: 829, time: 0.345, data: 0.002) G_GAN: 2.713 G_L1: 0.280 D_real: 0.134 D_fake: 0.313 \n",
            "(epoch: 172, iters: 1829, time: 0.209, data: 0.002) G_GAN: 4.638 G_L1: 0.510 D_real: 0.011 D_fake: 0.027 \n",
            "End of epoch 172 / 200 \t Time Taken: 66 sec\n",
            "learning rate 0.0000554 -> 0.0000535\n",
            "(epoch: 173, iters: 828, time: 0.228, data: 0.001) G_GAN: 2.969 G_L1: 0.385 D_real: 0.010 D_fake: 0.286 \n",
            "saving the latest model (epoch 173, total_iters 345000)\n",
            "(epoch: 173, iters: 1828, time: 0.224, data: 0.006) G_GAN: 2.158 G_L1: 0.364 D_real: 0.019 D_fake: 0.634 \n",
            "End of epoch 173 / 200 \t Time Taken: 67 sec\n",
            "learning rate 0.0000535 -> 0.0000515\n",
            "(epoch: 174, iters: 827, time: 0.368, data: 0.002) G_GAN: 4.036 G_L1: 0.493 D_real: 0.120 D_fake: 0.035 \n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f673f8459e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1301, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 52, in <module>\n",
            "    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py\", line 126, in optimize_parameters\n",
            "    self.backward_G()                   # calculate graidents for G\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py\", line 108, in backward_G\n",
            "    pred_fake = self.netD(fake_AB)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/data_parallel.py\", line 166, in forward\n",
            "    return self.module(*inputs[0], **kwargs[0])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/networks.py\", line 583, in forward\n",
            "    return self.model(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 141, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
            "    self.padding, self.dilation, self.groups)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pix2pix",
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-gpu.2-3.m74",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}